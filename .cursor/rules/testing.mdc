# ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã¨pytestä½¿ç”¨ãƒ«ãƒ¼ãƒ«

## ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰
```
    ğŸ”º E2E Tests (å°‘æ•°)
   ğŸ”ºğŸ”º Integration Tests (ä¸­ç¨‹åº¦)
  ğŸ”ºğŸ”ºğŸ”º Unit Tests (å¤šæ•°)
```

### ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«
1. **Unit Tests** - å€‹åˆ¥é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆ
2. **Integration Tests** - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®é€£æºãƒ†ã‚¹ãƒˆ
3. **E2E Tests** - ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å‹•ä½œãƒ†ã‚¹ãƒˆ

### ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™
- **å…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸: 80%ä»¥ä¸Š**
- **æ–°è¦ã‚³ãƒ¼ãƒ‰: 90%ä»¥ä¸Š**
- **ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹: 100%**

## pytestè¨­å®š

### pyproject.tomlè¨­å®š
```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config", 
    "--verbose",
    "--tb=short",
    "--cov=src",
    "--cov-report=html:htmlcov",
    "--cov-report=term-missing",
    "--cov-report=xml",
    "--cov-fail-under=80",
    "--durations=10"
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "selenium: marks tests that use selenium",
    "docker: marks tests that require docker"
]
```

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
```
tests/
â”œâ”€â”€ conftest.py              # å…±é€šfixtureå®šç¾©
â”œâ”€â”€ unit/                    # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_main.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ integration/             # çµ±åˆãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_selenium.py
â”‚   â””â”€â”€ test_docker.py
â””â”€â”€ fixtures/                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    â”œâ”€â”€ sample_data.json
    â””â”€â”€ mock_responses.py
```

## ãƒ†ã‚¹ãƒˆä½œæˆãƒ«ãƒ¼ãƒ«

### AAA ãƒ‘ã‚¿ãƒ¼ãƒ³
ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã¯ä»¥ä¸‹ã®æ§‹é€ ã«å¾“ã†ï¼š

```python
def test_function_behavior() -> None:
    """Test that function behaves correctly with valid input."""
    # Arrange - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒƒã‚¯ã®æº–å‚™
    input_data = {"key": "value"}
    expected_result = {"processed": True}
    
    # Act - ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®å®Ÿè¡Œ
    result = target_function(input_data)
    
    # Assert - çµæœã®æ¤œè¨¼
    assert result == expected_result
```

### å‘½åè¦å‰‡
```python
# âœ… Good: å…·ä½“çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„
def test_main_function_returns_success_when_selenium_runs_correctly() -> None:
    pass

def test_main_function_raises_webdriver_exception_when_chrome_not_found() -> None:
    pass

# âŒ Bad: æŠ½è±¡çš„ã§åˆ†ã‹ã‚Šã«ãã„
def test_main() -> None:
    pass

def test_error_case() -> None:
    pass
```

### ãƒ†ã‚¹ãƒˆé–¢æ•°ã®æ§‹é€ 
```python
import pytest
from typing import Any
from unittest.mock import Mock, patch

def test_selenium_driver_initialization_success() -> None:
    """Test successful Chrome WebDriver initialization."""
    # Arrange
    expected_options = ["--headless", "--no-sandbox", "--disable-dev-shm-usage"]
    
    # Act & Assert
    with patch("selenium.webdriver.Chrome") as mock_chrome:
        mock_driver = Mock()
        mock_chrome.return_value = mock_driver
        
        driver = initialize_driver()
        
        # Verify driver was created
        mock_chrome.assert_called_once()
        assert driver == mock_driver
        
        # Verify options were set
        call_args = mock_chrome.call_args
        options = call_args[1]["options"]
        for option in expected_options:
            assert option in options.arguments
```

## Fixture ä½¿ç”¨æ–¹æ³•

### conftest.py ã§ã®å…±é€šfixtureå®šç¾©
```python
import pytest
from typing import Generator, Dict, Any
from unittest.mock import Mock, patch
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

@pytest.fixture(scope="session")
def chrome_options() -> Options:
    """Chrome WebDriver options for testing."""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    return options

@pytest.fixture(scope="function")
def mock_webdriver() -> Generator[Mock, None, None]:
    """Mock WebDriver for testing."""
    with patch("selenium.webdriver.Chrome") as mock_driver:
        mock_instance = Mock()
        mock_driver.return_value = mock_instance
        yield mock_instance

@pytest.fixture(scope="function")
def sample_scraped_data() -> Dict[str, Any]:
    """Sample data for testing scrapers."""
    return {
        "title": "Test Article",
        "url": "https://example.com/test",
        "content": "Test content",
        "timestamp": "2024-01-01T00:00:00Z"
    }

@pytest.fixture(scope="function")
def temp_file() -> Generator[str, None, None]:
    """Create temporary file for testing."""
    import tempfile
    import os
    
    fd, path = tempfile.mkstemp()
    try:
        yield path
    finally:
        os.close(fd)
        os.unlink(path)
```

### Fixture ã®ä½¿ç”¨ä¾‹
```python
def test_scraper_with_mock_driver(mock_webdriver: Mock, sample_scraped_data: Dict[str, Any]) -> None:
    """Test scraper with mocked WebDriver."""
    # Arrange
    mock_webdriver.get.return_value = None
    mock_webdriver.find_element.return_value.text = sample_scraped_data["title"]
    
    # Act
    result = scrape_article("https://example.com/test")
    
    # Assert
    assert result["title"] == sample_scraped_data["title"]
    mock_webdriver.get.assert_called_once_with("https://example.com/test")
```

## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆ

### pytest.mark.parametrize ã®ä½¿ç”¨
```python
@pytest.mark.parametrize("input_url,expected_domain", [
    ("https://example.com/article", "example.com"),
    ("https://test.org/news", "test.org"),
    ("http://localhost:8000/", "localhost"),
])
def test_extract_domain_from_url(input_url: str, expected_domain: str) -> None:
    """Test domain extraction from various URL formats."""
    result = extract_domain(input_url)
    assert result == expected_domain

@pytest.mark.parametrize("invalid_url", [
    "",
    "not-a-url",
    "ftp://example.com",
    None,
])
def test_extract_domain_raises_error_for_invalid_urls(invalid_url: str) -> None:
    """Test that invalid URLs raise appropriate errors."""
    with pytest.raises(ValueError):
        extract_domain(invalid_url)
```

## ä¾‹å¤–ãƒ†ã‚¹ãƒˆ

### pytest.raises ã®ä½¿ç”¨
```python
def test_function_raises_specific_exception() -> None:
    """Test that function raises specific exception with correct message."""
    with pytest.raises(ValueError, match="Invalid input parameter"):
        risky_function("invalid_input")

def test_function_raises_exception_with_custom_attributes() -> None:
    """Test exception with custom attributes."""
    with pytest.raises(CustomException) as exc_info:
        function_that_raises_custom_exception()
    
    assert exc_info.value.error_code == 400
    assert "custom message" in str(exc_info.value)
```

## ãƒ¢ãƒƒã‚¯ã¨ãƒ‘ãƒƒãƒ

### unittest.mock ã®ä½¿ç”¨
```python
from unittest.mock import Mock, patch, MagicMock, call

def test_function_with_external_api_call() -> None:
    """Test function that makes external API calls."""
    with patch("requests.get") as mock_get:
        # Arrange
        mock_response = Mock()
        mock_response.json.return_value = {"status": "success"}
        mock_response.status_code = 200
        mock_get.return_value = mock_response
        
        # Act
        result = fetch_data_from_api("https://api.example.com/data")
        
        # Assert
        assert result["status"] == "success"
        mock_get.assert_called_once_with("https://api.example.com/data")

@patch("src.main.webdriver.Chrome")
def test_selenium_driver_quit_called(mock_chrome: Mock) -> None:
    """Test that WebDriver quit is properly called."""
    # Arrange
    mock_driver = Mock()
    mock_chrome.return_value = mock_driver
    
    # Act
    with selenium_driver() as driver:
        pass  # Context manager should handle quit
    
    # Assert
    mock_driver.quit.assert_called_once()
```

## éåŒæœŸãƒ†ã‚¹ãƒˆ

### pytest-asyncio ã®ä½¿ç”¨
```python
import pytest
import asyncio

@pytest.mark.asyncio
async def test_async_function() -> None:
    """Test asynchronous function."""
    # Arrange
    expected_result = {"data": "test"}
    
    # Act
    result = await async_fetch_data()
    
    # Assert
    assert result == expected_result

@pytest.mark.asyncio
async def test_async_function_with_timeout() -> None:
    """Test async function with timeout."""
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(slow_async_function(), timeout=1.0)
```

## ãƒãƒ¼ã‚«ãƒ¼ã®ä½¿ç”¨

### ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ã‚«ãƒ¼
```python
import pytest

@pytest.mark.slow
def test_large_data_processing() -> None:
    """Test that processes large amounts of data - marked as slow."""
    pass

@pytest.mark.integration
def test_selenium_integration() -> None:
    """Integration test with Selenium - requires browser."""
    pass

@pytest.mark.docker
def test_docker_container_functionality() -> None:
    """Test that requires Docker container."""
    pass

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã®é¸æŠ
# uv run pytest -m "not slow"              # slowãƒ†ã‚¹ãƒˆã‚’é™¤å¤–
# uv run pytest -m "integration"           # integrationãƒ†ã‚¹ãƒˆã®ã¿
# uv run pytest -m "not (slow or docker)"  # slow ã¨ docker ã‚’é™¤å¤–
```

## ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

### åŸºæœ¬å®Ÿè¡Œ
```bash
# å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
uv run pytest

# ç‰¹å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
uv run pytest tests/unit/

# ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«
uv run pytest tests/test_main.py

# ç‰¹å®šãƒ†ã‚¹ãƒˆé–¢æ•°
uv run pytest tests/test_main.py::test_main_function_success

# verboseå‡ºåŠ›
uv run pytest -v

# è©³ç´°å‡ºåŠ›
uv run pytest -vv
```

### ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ãå®Ÿè¡Œ
```bash
# ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
uv run pytest --cov=src

# HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
uv run pytest --cov=src --cov-report=html

# ä¸è¶³è¡Œè¡¨ç¤º
uv run pytest --cov=src --cov-report=term-missing

# ã‚«ãƒãƒ¬ãƒƒã‚¸é–¾å€¤ãƒã‚§ãƒƒã‚¯
uv run pytest --cov=src --cov-fail-under=80
```

### ä¸¦åˆ—å®Ÿè¡Œ
```bash
# CPUæ•°ã«å¿œã˜ãŸä¸¦åˆ—å®Ÿè¡Œ
uv run pytest -n auto

# æŒ‡å®šæ•°ã§ã®ä¸¦åˆ—å®Ÿè¡Œ
uv run pytest -n 4
```

### ãƒ‡ãƒãƒƒã‚°
```bash
# pdbã§ãƒ‡ãƒãƒƒã‚°
uv run pytest --pdb

# æœ€åˆã®å¤±æ•—ã§åœæ­¢
uv run pytest -x

# 2å›å¤±æ•—ã§åœæ­¢
uv run pytest --maxfail=2

# é…ã„ãƒ†ã‚¹ãƒˆã®è¡¨ç¤º
uv run pytest --durations=10
```

## CI/CD ã§ã®è¨­å®š

### GitHub Actionsä¾‹
```yaml
- name: Run tests
  run: |
    uv run pytest \
      --cov=src \
      --cov-report=xml \
      --cov-fail-under=80 \
      --junit-xml=test-results.xml
```

### ç¶™ç¶šçš„ãƒ†ã‚¹ãƒˆå“è³ªä¿è¨¼
- **ã‚«ãƒãƒ¬ãƒƒã‚¸ã®æ¼¸é€²çš„å‘ä¸Š**: å¾ã€…ã«é–¾å€¤ã‚’ä¸Šã’ã‚‹
- **ãƒ•ãƒ¬ã‚¤ã‚­ãƒ¼ãƒ†ã‚¹ãƒˆã®æ¤œå‡º**: åŒã˜ãƒ†ã‚¹ãƒˆã‚’è¤‡æ•°å›å®Ÿè¡Œ
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ**: `--durations` ã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
description:
globs:
alwaysApply: false
---
